{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75226455",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "import numpy as np\n",
    "\n",
    "def create_progress_evaluator_model():\n",
    "    \"\"\"\n",
    "    Crea un modelo para evaluar el progreso del usuario y predecir\n",
    "    la probabilidad de recordar una palabra.\n",
    "    \"\"\"\n",
    "    # Definir el modelo\n",
    "    model = Sequential([\n",
    "    # Capa de entrada - 3 características\n",
    "    Dense(16, activation='relu', input_shape=(3,)),\n",
    "    Dropout(0.2), # Prevenir overfitting\n",
    "    # Capa oculta\n",
    "    Dense(8, activation='relu'),\n",
    "    Dropout(0.1),\n",
    "    # Capa de salida - probabilidad de recordar la palabra\n",
    "    Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    # Compilar el modelo\n",
    "    model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "# Crear el modelo\n",
    "progress_model = create_progress_evaluator_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0bf25314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "96134/96134 [==============================] - 91s 936us/step - loss: 0.2586 - accuracy: 0.8517 - val_loss: 0.2332 - val_accuracy: 0.8534\n",
      "Epoch 2/20\n",
      "96134/96134 [==============================] - 95s 990us/step - loss: 0.2390 - accuracy: 0.8540 - val_loss: 0.2339 - val_accuracy: 0.8506\n",
      "Epoch 3/20\n",
      "96134/96134 [==============================] - 87s 904us/step - loss: 0.2370 - accuracy: 0.8540 - val_loss: 0.2317 - val_accuracy: 0.8540\n",
      "Epoch 4/20\n",
      "96134/96134 [==============================] - 90s 939us/step - loss: 0.2362 - accuracy: 0.8541 - val_loss: 0.2330 - val_accuracy: 0.8532\n",
      "Epoch 5/20\n",
      "96134/96134 [==============================] - 83s 867us/step - loss: 0.2358 - accuracy: 0.8541 - val_loss: 0.2315 - val_accuracy: 0.8535\n",
      "Epoch 6/20\n",
      "96134/96134 [==============================] - 96s 1ms/step - loss: 0.2356 - accuracy: 0.8541 - val_loss: 0.2337 - val_accuracy: 0.8534\n",
      "Epoch 7/20\n",
      "96134/96134 [==============================] - 93s 971us/step - loss: 0.2354 - accuracy: 0.8541 - val_loss: 0.2321 - val_accuracy: 0.8521\n",
      "Epoch 8/20\n",
      "96134/96134 [==============================] - 96s 994us/step - loss: 0.2352 - accuracy: 0.8541 - val_loss: 0.2327 - val_accuracy: 0.8546\n",
      "Epoch 9/20\n",
      "96134/96134 [==============================] - 86s 897us/step - loss: 0.2351 - accuracy: 0.8541 - val_loss: 0.2332 - val_accuracy: 0.8527\n",
      "Epoch 10/20\n",
      "96134/96134 [==============================] - 83s 865us/step - loss: 0.2351 - accuracy: 0.8541 - val_loss: 0.2307 - val_accuracy: 0.8543\n",
      "Epoch 11/20\n",
      "96134/96134 [==============================] - 81s 845us/step - loss: 0.2349 - accuracy: 0.8540 - val_loss: 0.2338 - val_accuracy: 0.8529\n",
      "Epoch 12/20\n",
      "96134/96134 [==============================] - 85s 881us/step - loss: 0.2350 - accuracy: 0.8540 - val_loss: 0.2306 - val_accuracy: 0.8527\n",
      "Epoch 13/20\n",
      "96134/96134 [==============================] - 88s 917us/step - loss: 0.2350 - accuracy: 0.8540 - val_loss: 0.2326 - val_accuracy: 0.8512\n",
      "Epoch 14/20\n",
      "96134/96134 [==============================] - 80s 837us/step - loss: 0.2349 - accuracy: 0.8540 - val_loss: 0.2317 - val_accuracy: 0.8522\n",
      "Epoch 15/20\n",
      "96134/96134 [==============================] - 81s 847us/step - loss: 0.2348 - accuracy: 0.8541 - val_loss: 0.2299 - val_accuracy: 0.8529\n",
      "Epoch 16/20\n",
      "96134/96134 [==============================] - 82s 853us/step - loss: 0.2349 - accuracy: 0.8540 - val_loss: 0.2321 - val_accuracy: 0.8544\n",
      "Epoch 17/20\n",
      "96134/96134 [==============================] - 81s 847us/step - loss: 0.2348 - accuracy: 0.8541 - val_loss: 0.2305 - val_accuracy: 0.8545\n",
      "Epoch 18/20\n",
      "96134/96134 [==============================] - 82s 850us/step - loss: 0.2347 - accuracy: 0.8541 - val_loss: 0.2311 - val_accuracy: 0.8543\n",
      "Epoch 19/20\n",
      "96134/96134 [==============================] - 82s 856us/step - loss: 0.2347 - accuracy: 0.8540 - val_loss: 0.2298 - val_accuracy: 0.8543\n",
      "Epoch 20/20\n",
      "96134/96134 [==============================] - 81s 840us/step - loss: 0.2346 - accuracy: 0.8541 - val_loss: 0.2326 - val_accuracy: 0.8520\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "def generate_synthetic_training_data(num_samples=1000):\n",
    "    \"\"\"\n",
    "    Genera datos sintéticos para el entrenamiento inicial del modelo.\n",
    "    \"\"\"\n",
    "    X = np.zeros((num_samples, 7))\n",
    "    y = np.zeros((num_samples, 1))\n",
    "    for i in range(num_samples):\n",
    "    # Generar características aleatorias\n",
    "    times_reviewed = np.random.randint(1, 10)\n",
    "    times_correct = np.random.randint(0, times_reviewed + 1)\n",
    "    times_incorrect = times_reviewed - times_correct\n",
    "    days_since_last = np.random.randint(0, 30)\n",
    "    avg_response_time = np.random.uniform(1.0, 10.0)\n",
    "    word_difficulty = np.random.randint(1, 6)\n",
    "    word_level_idx = np.random.randint(0, 4) # 0:A1, 1:A2, 2:B1, 3:B2\n",
    "    # Probabilidad \"real\" (sintética) basada en reglas lógicas\n",
    "    p_remember = 0.9 * (times_correct / max(1, times_reviewed)) - \\\n",
    "    0.05 * days_since_last - \\\n",
    "    0.1 * word_difficulty + \\\n",
    "    0.2 * (1.0 - min(1.0, avg_response_time / 10.0))\n",
    "    # Ajustar a rango [0, 1]\n",
    "    p_remember = max(0.01, min(0.99, p_remember))\n",
    "    # Asignar valores\n",
    "    X[i] = [times_reviewed, times_correct, times_incorrect,\n",
    "    days_since_last, times_correct/max(1, times_reviewed),\n",
    "    avg_response_time, word_difficulty]\n",
    "    # El objetivo es 1 si se recuerda, 0 si no\n",
    "    y[i] = 1 if np.random.random() < p_remember else 0\n",
    "    return X, y\n",
    "'''\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('duolingo.csv')\n",
    "\n",
    "# Generar datos sintéticos\n",
    "X_train, y_train = df.loc[:,(\"times_reviewed\",\"times_correct\",\"delta\")],df.loc[:,\"recall\"] \n",
    "# Entrenamiento inicial\n",
    "progress_model.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.2)\n",
    "# Guardar el modelo\n",
    "progress_model.save('vocabulary_progress_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "979a45d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estructura de datos de entrada\n",
    "\n",
    "user_word_interaction = {\n",
    " \"user_id\": \"user123\",\n",
    " \"word_id\": \"word456\",\n",
    " \"features\": {\n",
    " \"times_reviewed\": 3, # Número de veces que ha visto la palabra\n",
    " \"times_correct\": 2, # Veces que respondió correctamente\n",
    " \"delta\": 2, # Días desde la última revisión\n",
    " }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76f3a3d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6727309823036194"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def determine_review_priority(user_words_data):\n",
    "    \"\"\"\n",
    "    Determina qué palabras necesitan ser repasadas con mayor prioridad.\n",
    "    Args:\n",
    "    user_words_data: Lista de diccionarios con datos de interacción\n",
    "    Returns:\n",
    "    Lista de IDs de palabras ordenadas por prioridad de repaso\n",
    "    \"\"\"\n",
    "    word_priorities = []\n",
    "    for word_data in user_words_data:\n",
    "    recall_prob = predict_recall_probability(word_data)\n",
    "    # Calcular prioridad: palabras con baja probabilidad de ser recordadas\n",
    "    # pero que han sido revisadas tienen alta prioridad\n",
    "    priority_score = (1 - recall_prob) * (word_data[\"times_reviewed\"] + 1)\n",
    "    word_priorities.append({\n",
    "    \"word_id\": word_data[\"word_id\"],\n",
    "    \"recall_probability\": recall_prob,\n",
    "    \"priority_score\": priority_score\n",
    "    })\n",
    "    # Ordenar por prioridad (mayor primero)\n",
    "    sorted_priorities = sorted(word_priorities,\n",
    "    key=lambda x: x[\"priority_score\"],\n",
    "    reverse=True)\n",
    "    return [item[\"word_id\"] for item in sorted_priorities]\n",
    "\n",
    "'''\n",
    "\n",
    "def predict_recall_probability(user_word_data):\n",
    "    \"\"\"\n",
    "    Predice la probabilidad de que un usuario recuerde una palabra\n",
    "    basado en sus interacciones previas.\n",
    "    Args:\n",
    "    user_word_data: Dict con los datos de interacción usuario-palabra\n",
    "    Returns:\n",
    "    Probabilidad (0-1) de que el usuario recuerde la palabra\n",
    "    \"\"\"\n",
    "    # Extraer características\n",
    "    features = [\n",
    "    user_word_data[\"times_reviewed\"],\n",
    "    user_word_data[\"times_correct\"],\n",
    "    user_word_data[\"delta\"],\n",
    "    ]\n",
    "    # Convertir a array y dar forma adecuada\n",
    "    features_array = np.array(features).reshape(1, -1)\n",
    "    # Realizar predicción\n",
    "    probability = progress_model.predict(features_array)[0][0]\n",
    "    return float(probability)\n",
    "\n",
    "\n",
    "predict_recall_probability(user_word_interaction[\"features\"])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
